# knowledge_base.py

data_ml = {
    # =========================================================
    # REGRESI√ìN (Predecir Valores Num√©ricos)
    # =========================================================
    
    "Lasso": [
        {
            "titulo": "üè† 1. Precio de Casas (Feature Selection)",
            "contexto": "Predecir el precio de una casa eliminando variables basura (como el color de la puerta).",
            "pasos": [
                "1. Importamos Lasso y Numpy.",
                "2. Creamos datos: [Metros2, Habitaciones, Color(1-10)].",
                "3. Entrenamos con un alpha (castigo) bajo.",
                "4. Lasso vuelve 0 el coeficiente del 'Color'."
            ],
            "codigo": """import numpy as np
from sklearn.linear_model import Lasso

# Datos: [Metros2, Habitaciones, Color(Ruido)]
X = np.array([
    [100, 2, 5], 
    [150, 3, 1], 
    [200, 4, 9], 
    [120, 2, 2]
])
y = np.array([200000, 300000, 400000, 240000])

# Alpha controla la severidad. 0.1 elimina ruido leve.
modelo = Lasso(alpha=0.1)
modelo.fit(X, y)

print("Predicci√≥n (180m2, 3hab, color 5):")
print(f"${modelo.predict([[180, 3, 5]])[0]:,.2f}")

print("Importancia de variables:", modelo.coef_)
# Nota c√≥mo el √∫ltimo valor (Color) se acerca a 0""",
            
            "explicacion_codigo": "Definimos 'X' como nuestra matriz de caracter√≠sticas y 'y' como el precio objetivo. Al llamar a .coef_, vemos qu√© peso le dio el modelo a cada columna.",
            "quiz": {
                "pregunta": "¬øQu√© hace Lasso con las variables irrelevantes?",
                "opciones": ["Las multiplica por 10", "Reduce su peso a CERO", "Da un error"],
                "correcta": "Reduce su peso a CERO",
                "retro_acierto": "¬°Exacto! Lasso limpia tus datos eliminando lo que no sirve.",
                "retro_fallo": "Incorrecto. Lasso se caracteriza por 'borrar' variables (peso cero), no por multiplicarlas ni fallar."
            },
            "versus": {
                "rival": "Ridge Regression",
                "comparacion": "Lasso ELIMINA variables (coeficiente 0). Ridge solo las hace muy peque√±as, pero las mantiene todas."
            }
        },
        {
            "titulo": "üåæ 2. Rendimiento de Cultivos",
            "contexto": "Predecir toneladas de cosecha ignorando mitos (fase lunar).",
            "pasos": [
                "1. Datos: Lluvia, Fertilizante, Fase Lunar.", 
                "2. Lasso detecta que la Luna no afecta.", 
                "3. Predice cosecha basada solo en lo real."
            ],
            "codigo": """import numpy as np
from sklearn.linear_model import Lasso

# Datos: [Lluvia(mm), Fertilizante(kg), FaseLunar(0-1)]
X = np.array([
    [100, 50, 1], 
    [200, 60, 0], 
    [50, 20, 1], 
    [300, 80, 0]
])
y = np.array([10, 25, 5, 35]) # Toneladas

modelo = Lasso(alpha=0.1)
modelo.fit(X, y)

# Predecir con mucha lluvia (150mm) y abono (55kg)
pred = modelo.predict([[150, 55, 1]])
print(f"Cosecha estimada: {pred[0]:.2f} toneladas")""",
            
            "explicacion_codigo": "Aunque le pasamos el dato de la fase lunar (el 1 al final), el modelo matem√°tico aprende a ignorarlo para calcular las toneladas.",
            "quiz": {
                "pregunta": "¬øPor qu√© usar Lasso y no Regresi√≥n Lineal aqu√≠?",
                "opciones": ["Para ignorar el ruido", "Es m√°s r√°pido", "Es m√°s bonito"],
                "correcta": "Para ignorar el ruido",
                "retro_acierto": "¬°Correcto! La regresi√≥n normal intentar√≠a usar la fase lunar, cometiendo errores.",
                "retro_fallo": "No es eso. La clave de este ejemplo es que tenemos datos basura (ruido) que queremos ignorar."
            },
            "versus": {
                "rival": "Linear Regression",
                "comparacion": "La regresi√≥n simple se confunde f√°cil con datos basura. Lasso es inmune al ruido."
            }
        },
        {
            "titulo": "üöó 3. Valor de Autos Usados",
            "contexto": "Estimar precio ignorando accesorios est√©ticos sin valor.",
            "pasos": ["1. Variables: Km, A√±o, Calcoman√≠as.", "2. Lasso ignora las calcoman√≠as.", "3. Predice precio."],
            "codigo": """import numpy as np
from sklearn.linear_model import Lasso

# [Km, A√±o, Calcoman√≠as(1=S√≠/0=No)]
X = np.array([
    [50000, 2015, 1], 
    [10000, 2020, 0], 
    [80000, 2012, 1]
])
y = np.array([15000, 25000, 8000]) # Precios

modelo = Lasso(alpha=1.0)
modelo.fit(X, y)

nuevo_auto = [[60000, 2016, 0]]
print(f"Precio estimado: ${modelo.predict(nuevo_auto)[0]:.2f}")""",
            
            "explicacion_codigo": "Usamos un alpha=1.0 para ser m√°s estrictos. El modelo penaliza la complejidad para evitar sobreajustarse a los datos de entrenamiento.",
            "quiz": {
                "pregunta": "¬øQu√© par√°metro controla el castigo en Lasso?",
                "opciones": ["Beta", "Alpha", "Gamma"],
                "correcta": "Alpha",
                "retro_acierto": "¬°Bien! Alpha es el hiperpar√°metro clave.",
                "retro_fallo": "Incorrecto. Beta y Gamma se usan en otros modelos, aqu√≠ usamos Alpha."
            },
            "versus": {
                "rival": "ElasticNet",
                "comparacion": "ElasticNet mezcla Lasso y Ridge. √ösalo si Lasso borra demasiadas cosas."
            }
        }
    ],

    "SVR": [
        {
            "titulo": "üìà 1. Tendencia de Acciones",
            "contexto": "Ajustar una curva a precios que suben y bajan (no lineal).",
            "pasos": ["1. Datos de d√≠as.", "2. Kernel RBF permite curvas.", "3. Predicci√≥n futura."],
            "codigo": """import numpy as np
from sklearn.svm import SVR

# D√≠a 1, D√≠a 2, D√≠a 3...
X = np.array([[1], [2], [3], [4], [5]])
# Precios (suben y bajan)
y = np.array([100, 110, 105, 115, 120])

# Kernel 'rbf' permite ajustar l√≠neas curvas
modelo = SVR(kernel='rbf', C=100, gamma=0.1)
modelo.fit(X, y)

dia_futuro = [[6]]
print(f"Precio estimado D√≠a 6: {modelo.predict(dia_futuro)[0]:.2f}")""",
            
            "explicacion_codigo": "kernel='rbf' (Radial Basis Function) es lo que permite doblar la l√≠nea. 'C' controla cu√°nto queremos evitar errores (C alto = ajuste estricto).",
            "quiz": {
                "pregunta": "¬øQu√© permite a SVR hacer curvas?",
                "opciones": ["El Kernel", "El precio", "La memoria"],
                "correcta": "El Kernel",
                "retro_acierto": "¬°Exacto! El 'Truco del Kernel' proyecta datos para hallar patrones curvos.",
                "retro_fallo": "No. El precio es el dato y la memoria es hardware. La matem√°tica curva viene del Kernel."
            },
            "versus": {
                "rival": "Linear Regression",
                "comparacion": "La regresi√≥n lineal solo dibuja rectas. SVR dibuja curvas complejas."
            }
        },
        {
            "titulo": "üå°Ô∏è 2. Predicci√≥n de Temperatura",
            "contexto": "Relaci√≥n compleja entre humedad y calor.",
            "pasos": ["1. Datos hist√≥ricos.", "2. Kernel Polinomial (curva U).", "3. Predice grados."],
            "codigo": """import numpy as np
from sklearn.svm import SVR

# Humedad (%)
X = np.array([[20], [30], [40], [80], [90]])
# Grados Celsius
y = np.array([35, 32, 30, 15, 12])

# Degree=2 intenta ajustar una par√°bola
modelo = SVR(kernel='poly', degree=2) 
modelo.fit(X, y)

print(f"Temp estimada con 50% humedad: {modelo.predict([[50]])[0]:.1f}¬∞C")""",
            
            "explicacion_codigo": "Degree=2 significa que buscamos una relaci√≥n cuadr√°tica (x¬≤), √∫til para fen√≥menos f√≠sicos que aceleran o desaceleran.",
            "quiz": {
                "pregunta": "¬øQu√© es Epsilon en SVR?",
                "opciones": ["Margen de error tolerado", "Nombre del creador", "Velocidad"],
                "correcta": "Margen de error tolerado",
                "retro_acierto": "¬°As√≠ es! SVR crea un 'tubo' de tolerancia alrededor de la predicci√≥n.",
                "retro_fallo": "Falso. Epsilon define qu√© tan exigente es el modelo con la precisi√≥n exacta."
            },
            "versus": {
                "rival": "Decision Trees",
                "comparacion": "Los √°rboles hacen predicciones escalonadas. SVR hace curvas suaves."
            }
        },
        {
            "titulo": "üèóÔ∏è 3. Resistencia de Materiales",
            "contexto": "Peso m√°ximo de una viga seg√∫n grosor (Relaci√≥n Lineal).",
            "pasos": ["1. Datos ingenier√≠a.", "2. SVR Lineal.", "3. Carga m√°xima."],
            "codigo": """import numpy as np
from sklearn.svm import SVR

# Grosor en mm
X = np.array([[10], [20], [30], [40]])
# Carga soportada en kg
y = np.array([100, 200, 300, 400])

# Aqu√≠ la relaci√≥n es directa, usamos linear
modelo = SVR(kernel='linear')
modelo.fit(X, y)

print(f"Carga para 25mm: {modelo.predict([[25]])[0]:.0f} kg")""",
            
            "explicacion_codigo": "Aunque SVR es famoso por curvas, con kernel='linear' funciona igual que una regresi√≥n pero m√°s robusta ante outliers.",
            "quiz": {
                "pregunta": "Si los datos forman una recta, ¬øqu√© Kernel usas?",
                "opciones": ["RBF", "Poly", "Linear"],
                "correcta": "Linear",
                "retro_acierto": "¬°L√≥gico! No gastes recursos en curvas si una recta funciona.",
                "retro_fallo": "No. RBF y Poly son para curvas complejas. Si es recto, usa Linear."
            },
            "versus": {
                "rival": "Lasso",
                "comparacion": "Ambos hacen rectas, pero SVR ignora mejor los errores peque√±os dentro del margen."
            }
        }
    ],

    "SGD Regressor": [
        {
            "titulo": "‚ö° 1. Consumo El√©ctrico Masivo",
            "contexto": "Aprender dato por dato sin llenar la memoria RAM (Streaming).",
            "pasos": ["1. Llega un lote de datos.", "2. partial_fit actualiza modelo.", "3. Olvida esos datos."],
            "codigo": """import numpy as np
from sklearn.linear_model import SGDRegressor

# Configuramos el modelo
modelo = SGDRegressor(max_iter=1000, tol=1e-3)

print("Iniciando aprendizaje por streaming...")

# Simulamos que llegan datos de 3 en 3 (mini-batches)
for i in range(3):
    # Generamos 3 datos aleatorios nuevos
    X_chunk = np.random.rand(3, 1) 
    y_chunk = 2 * X_chunk.flatten() + 1 
    
    # partial_fit NO reinicia el modelo, solo lo actualiza
    modelo.partial_fit(X_chunk, y_chunk)
    print(f"Ronda {i+1}: Pesos actualizados -> {modelo.coef_}")

print("Modelo listo para seguir recibiendo datos.")""",
            
            "explicacion_codigo": "La funci√≥n clave es .partial_fit(). A diferencia de .fit() normal, esta permite entrenar el modelo poco a poco, ideal para Big Data.",
            "quiz": {
                "pregunta": "¬øPor qu√© usar SGD en lugar de Regresi√≥n normal?",
                "opciones": ["Es m√°s preciso", "Soporta datos infinitos", "Es m√°s f√°cil"],
                "correcta": "Soporta datos infinitos",
                "retro_acierto": "¬°Correcto! Nunca carga todos los datos a la vez en la RAM.",
                "retro_fallo": "No exactamente. Su gran ventaja es procesar datos que no caben en memoria."
            },
            "versus": {
                "rival": "Batch Gradient Descent",
                "comparacion": "Batch lee TODOS los datos para dar un paso. SGD da un paso con cada dato (m√°s r√°pido)."
            }
        },
        {
            "titulo": "üñ±Ô∏è 2. Predicci√≥n de Clics (CTR)",
            "contexto": "Predecir probabilidad de click en publicidad online en tiempo real.",
            "pasos": ["1. Info del usuario.", "2. Actualizar al instante.", "3. Predecir."],
            "codigo": """import numpy as np
from sklearn.linear_model import SGDRegressor

modelo = SGDRegressor()

# Datos simulados: [Edad, Hora del d√≠a]
X_usuario1 = np.array([[25, 10]])
y_usuario1 = np.array([1]) # Dio Click

# El modelo aprende de este usuario √∫nico
modelo.partial_fit(X_usuario1, y_usuario1)

# Llega usuario nuevo
X_nuevo = np.array([[30, 12]])
print("Predicci√≥n para nuevo usuario:", modelo.predict(X_nuevo))""",
            
            "explicacion_codigo": "En publicidad, los gustos cambian r√°pido. SGD permite re-entrenar el modelo cada segundo con la actividad m√°s reciente.",
            "quiz": {
                "pregunta": "¬øQu√© pasa si los datos llegan muy r√°pido?",
                "opciones": ["SGD se bloquea", "SGD se adapta r√°pido", "Necesitas reiniciar"],
                "correcta": "SGD se adapta r√°pido",
                "retro_acierto": "¬°Exacto! Es ideal para sistemas de alta velocidad.",
                "retro_fallo": "Al contrario, SGD est√° dise√±ado para no bloquearse con velocidad."
            },
            "versus": {
                "rival": "Random Forest",
                "comparacion": "Un bosque es muy lento para re-entrenar. SGD lo hace en milisegundos."
            }
        },
        {
            "titulo": "üê¶ 3. Tendencias Twitter",
            "contexto": "Predecir volumen de tweets. El modelo debe adaptarse si algo se vuelve viral.",
            "pasos": ["1. Flujo de tweets.", "2. Ajustar pendiente.", "3. Predecir."],
            "codigo": """import numpy as np
from sklearn.linear_model import SGDRegressor

modelo = SGDRegressor()

# Minuto 1: 100 tweets
modelo.partial_fit([[1]], [100]) 
# Minuto 2: 200 tweets (Tendencia subiendo)
modelo.partial_fit([[2]], [200]) 

# ¬øCu√°ntos habr√° en el minuto 3?
pred = modelo.predict([[3]])
print(f"Predicci√≥n Minuto 3: {pred[0]:.0f} tweets")""",
            
            "explicacion_codigo": "El modelo calcula la pendiente (crecimiento) bas√°ndose solo en los √∫ltimos puntos recibidos.",
            "quiz": {
                "pregunta": "¬øQu√© significa 'Estoc√°stico'?",
                "opciones": ["Aleatorio / Al azar", "Est√°tico", "Estad√≠stico"],
                "correcta": "Aleatorio / Al azar",
                "retro_acierto": "¬°Bien! Toma muestras al azar para decidir hacia d√≥nde moverse (optimizar).",
                "retro_fallo": "No. Viene de 'Stochastic' que implica azar/probabilidad en el movimiento."
            },
            "versus": {
                "rival": "Standard SVR",
                "comparacion": "SVR necesita todo el historial. SGD solo necesita el √∫ltimo dato."
            }
        }
    ],

    # =========================================================
    # CLASIFICACI√ìN (Categor√≠as)
    # =========================================================

    "Naive Bayes": [
        {
            "titulo": "üìß 1. Filtro de SPAM",
            "contexto": "Si aparece 'GRATIS', la probabilidad de SPAM sube.",
            "pasos": ["1. Contar palabras.", "2. Calcular probabilidades.", "3. Clasificar."],
            "codigo": """import numpy as np
from sklearn.naive_bayes import MultinomialNB

# 0='Hola', 1='Gratis', 2='Reuni√≥n'
# Frase: "Hola Reunion" -> [1, 0, 1]
X = np.array([
    [1, 0, 1], # Normal
    [0, 1, 0], # Spam ("Gratis")
    [1, 1, 0]  # Spam ("Hola Gratis")
])
y = np.array([0, 1, 1]) # 0=Normal, 1=Spam

modelo = MultinomialNB()
modelo.fit(X, y)

# Nueva frase solo con "Gratis"
es_spam = modelo.predict([[0, 1, 0]])
print("¬øEs Spam?:", "S√ç" if es_spam[0] == 1 else "NO")""",
            
            "explicacion_codigo": "MultinomialNB funciona contando frecuencias. Si 'Gratis' aparece mucho en correos marcados como Spam, el modelo aprende esa asociaci√≥n.",
            "quiz": {
                "pregunta": "¬øPor qu√© se llama 'Naive' (Ingenuo)?",
                "opciones": ["Es tonto", "Asume independencia", "Es nuevo"],
                "correcta": "Asume independencia",
                "retro_acierto": "¬°Correcto! Cree que las palabras no se relacionan entre s√≠.",
                "retro_fallo": "Incorrecto. Se le dice ingenuo porque simplifica el mundo asumiendo que nada est√° conectado."
            },
            "versus": {
                "rival": "Logistic Regression",
                "comparacion": "Logistic busca una f√≥rmula precisa. Bayes cuenta probabilidades, siendo m√°s r√°pido entrenando."
            }
        },
        {
            "titulo": "üòä 2. An√°lisis de Sentimientos",
            "contexto": "Saber si un comentario es Positivo o Negativo.",
            "pasos": ["1. Contar buenas/malas.", "2. Probabilidades.", "3. Tono."],
            "codigo": """from sklearn.naive_bayes import MultinomialNB
import numpy as np

# [Palabras_Buenas, Palabras_Malas]
X = np.array([
    [3, 0], # "Muy muy bueno"
    [0, 3], # "Mal mal horrible"
    [2, 1]  # "Bueno pero lento"
])
y = np.array(['Positivo', 'Negativo', 'Neutro'])

modelo = MultinomialNB()
modelo.fit(X, y)

# Comentario con 1 buena y 5 malas
print("Resultado:", modelo.predict([[1, 5]])[0])""",
            
            "explicacion_codigo": "El modelo balancea la evidencia. Aunque haya 1 palabra buena, las 5 malas pesan m√°s en la probabilidad condicional.",
            "quiz": {
                "pregunta": "¬øPara qu√© datos es mejor Naive Bayes?",
                "opciones": ["Im√°genes", "Texto y NLP", "Audio"],
                "correcta": "Texto y NLP",
                "retro_acierto": "¬°S√≠! Es el rey del procesamiento de texto r√°pido.",
                "retro_fallo": "No. Para im√°genes y audio se usan redes neuronales."
            },
            "versus": {
                "rival": "LSTM (Deep Learning)",
                "comparacion": "Una red neuronal entiende sarcasmo. Naive Bayes no, es literal."
            }
        },
        {
            "titulo": "ü©∫ 3. Diagn√≥stico M√©dico Simple",
            "contexto": "Gripe vs Alergia basado en s√≠ntomas (S√≠/No).",
            "pasos": ["1. S√≠ntomas binarios.", "2. BernoulliNB.", "3. Diagn√≥stico."],
            "codigo": """from sklearn.naive_bayes import BernoulliNB
import numpy as np

# [Fiebre, Estornudos] -> 1=S√≠, 0=No
X = np.array([
    [1, 0], # Solo fiebre -> Gripe
    [0, 1], # Solo estornudo -> Alergia
    [1, 1]  # Ambos -> Gripe
])
y = np.array(['Gripe', 'Alergia', 'Gripe'])

modelo = BernoulliNB()
modelo.fit(X, y)

paciente = [[1, 1]] # Tiene ambas
print("Diagn√≥stico:", modelo.predict(paciente)[0])""",
            
            "explicacion_codigo": "Usamos BernoulliNB porque los datos son binarios (True/False). Multinomial es para conteos (1, 2, 3...).",
            "quiz": {
                "pregunta": "Si tiene fiebre, ¬øqu√© hace el modelo?",
                "opciones": ["Llama al doctor", "Calcula Prob(Gripe | Fiebre)", "Nada"],
                "correcta": "Calcula Prob(Gripe | Fiebre)",
                "retro_acierto": "¬°Exacto! Aplica el Teorema de Bayes puro.",
                "retro_fallo": "No. El modelo matem√°tico solo calcula la probabilidad condicional."
            },
            "versus": {
                "rival": "Decision Tree",
                "comparacion": "Un √°rbol sigue reglas fijas. Bayes maneja incertidumbre y probabilidades."
            }
        }
    ],

    "Linear SVC": [
        {
            "titulo": "üõë 1. Clasificaci√≥n Lineal Estricta",
            "contexto": "Separar dos grupos con una l√≠nea recta perfecta.",
            "pasos": ["1. Puntos 2D.", "2. Buscar mejor l√≠nea (margen).", "3. Clasificar."],
            "codigo": """from sklearn.svm import LinearSVC
import numpy as np

# Coordenadas [X, Y]
X = np.array([[1, 1], [2, 2], [8, 8], [9, 9]])
y = np.array([0, 0, 1, 1]) # Clase 0 (Abajo), Clase 1 (Arriba)

modelo = LinearSVC()
modelo.fit(X, y)

# Punto intermedio
punto = [[1.5, 1.5]]
print("Clase predicha:", modelo.predict(punto)[0])""",
            
            "explicacion_codigo": "LinearSVC busca trazar una l√≠nea que maximice la distancia (margen) entre los puntos m√°s cercanos de ambos grupos.",
            "quiz": {
                "pregunta": "¬øQu√© es el 'Margen'?",
                "opciones": ["El error", "Espacio vac√≠o entre la l√≠nea y datos", "El borde"],
                "correcta": "Espacio vac√≠o entre la l√≠nea y datos",
                "retro_acierto": "¬°Bien! SVM busca la 'carretera' m√°s ancha posible.",
                "retro_fallo": "Incorrecto. El margen es la separaci√≥n segura entre los dos grupos."
            },
            "versus": {
                "rival": "KNN",
                "comparacion": "KNN mira vecindad. SVM mira fronteras y geometr√≠a."
            }
        },
        {
            "titulo": "üíª 2. Detecci√≥n de Malware",
            "contexto": "Separar archivos seguros de virus usando firmas binarias.",
            "pasos": ["1. Caracter√≠sticas binarias.", "2. Hiperplano separador.", "3. Clasificar."],
            "codigo": """from sklearn.svm import LinearSVC
import numpy as np

# [Usa red?, Modifica sistema?, Es oculto?]
X = np.array([
    [0,0,0], # Seguro
    [1,1,1], # Virus
    [0,1,0], # Seguro
    [1,0,1]  # Virus
])
y = np.array(['Seguro', 'Virus', 'Seguro', 'Virus'])

modelo = LinearSVC()
modelo.fit(X, y)

archivo = [[1, 0, 1]] # Red + Oculto
print("El archivo es:", modelo.predict(archivo)[0])""",
            
            "explicacion_codigo": "En muchas dimensiones, la 'l√≠nea' separadora se llama Hiperplano. SVM es excelente encontrando este plano √≥ptimo.",
            "quiz": {
                "pregunta": "¬øSi los datos no se pueden separar con recta?",
                "opciones": ["Falla", "Hace curvas", "Borra datos"],
                "correcta": "Falla",
                "retro_acierto": "¬°Correcto! Para curvas necesitas SVM con Kernel o Random Forest.",
                "retro_fallo": "No. Este modelo es estrictamente lineal. Para curvas necesitas Kernels."
            },
            "versus": {
                "rival": "RBF SVM",
                "comparacion": "LinearSVC es r√≠gido (rectas). RBF SVM es flexible (curvas) pero lento."
            }
        },
        {
            "titulo": "üçé 3. Clasificaci√≥n de Frutas",
            "contexto": "Manzanas vs Naranjas usando color y peso.",
            "pasos": ["1. Peso/Color.", "2. Ajuste lineal.", "3. Predicci√≥n."],
            "codigo": """from sklearn.svm import LinearSVC
import numpy as np

# [Peso(gr), Color(1=Rojo, 5=Naranja)]
X = np.array([
    [150, 1], [160, 1], # Manzanas
    [140, 5], [155, 5]  # Naranjas
]) 
y = np.array(['Manzana', 'Manzana', 'Naranja', 'Naranja'])

modelo = LinearSVC()
modelo.fit(X, y)

fruta = [[145, 5]]
print("Es una:", modelo.predict(fruta)[0])""",
            
            "explicacion_codigo": "El modelo aprende que el eje 'Color' es determinante. Si color es alto (5), cae del lado del hiperplano de Naranjas.",
            "quiz": {
                "pregunta": "¬øSirve para texto?",
                "opciones": ["No", "S√≠, excelente", "Solo im√°genes"],
                "correcta": "S√≠, excelente",
                "retro_acierto": "¬°As√≠ es! Funciona genial en espacios de alta dimensi√≥n como texto.",
                "retro_fallo": "Te sorprender√°, pero s√≠. Es uno de los mejores para clasificar textos."
            },
            "versus": {
                "rival": "Random Forest",
                "comparacion": "Forest usa reglas 'Si... entonces'. SVM usa geometr√≠a matem√°tica."
            }
        }
    ],

    "KNN": [
        {
            "titulo": "üé• 1. Recomendador de Pel√≠culas",
            "contexto": "Si a tus vecinos les gusta, a ti tambi√©n. (Vecinos Cercanos).",
            "pasos": ["1. Mapa de gustos.", "2. Buscar 3 vecinos.", "3. Votaci√≥n."],
            "codigo": """from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# [Nivel Acci√≥n, Nivel Romance] (Escala 1-10)
X = np.array([
    [1, 9], [2, 8], # Aman Romance
    [9, 1], [8, 2]  # Aman Acci√≥n
])
y = np.array(['Romance', 'Romance', 'Acci√≥n', 'Acci√≥n'])

# Mira a los 3 m√°s cercanos
modelo = KNeighborsClassifier(n_neighbors=3)
modelo.fit(X, y)

usuario = [[5, 5]] # Gustos neutros
print("Recomendaci√≥n:", modelo.predict(usuario)[0])""",
            
            "explicacion_codigo": "n_neighbors=3 indica que el algoritmo buscar√° los 3 puntos m√°s cercanos geom√©tricamente y tomar√° la decisi√≥n por mayor√≠a de votos.",
            "quiz": {
                "pregunta": "¬øQu√© es la 'K' en KNN?",
                "opciones": ["Kil√≥metros", "N√∫mero de vecinos", "Constante"],
                "correcta": "N√∫mero de vecinos",
                "retro_acierto": "¬°Exacto! K=1 copia al m√°s cercano. K=100 hace votaci√≥n masiva.",
                "retro_fallo": "No. K se refiere a la cantidad de puntos cercanos que consultaremos."
            },
            "versus": {
                "rival": "Matrix Factorization",
                "comparacion": "KNN busca usuarios similares. Netflix usa Factorizaci√≥n para hallar patrones ocultos."
            }
        },
        {
            "titulo": "üî¢ 2. Reconocimiento de D√≠gitos",
            "contexto": "Comparar imagen nueva con base de datos p√≠xel a p√≠xel.",
            "pasos": ["1. P√≠xeles.", "2. Distancia.", "3. Clasificar."],
            "codigo": """from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# P√≠xeles simples [Blanco, Negro, Blanco...]
X = np.array([
    [0,1,0], # Patr√≥n 1
    [1,1,1], # Patr√≥n 2
    [1,0,1]  # Patr√≥n 3
])
y = np.array(['Uno', 'Tachado', 'U'])

modelo = KNeighborsClassifier(n_neighbors=1)
modelo.fit(X, y)

print("Predicci√≥n [0,1,0]:", modelo.predict([[0, 1, 0]])[0])""",
            
            "explicacion_codigo": "KNN no 'aprende' una forma. Simplemente guarda todas las im√°genes y compara la nueva con todas las guardadas.",
            "quiz": {
                "pregunta": "¬øDesventaja de KNN con muchos datos?",
                "opciones": ["Es lento", "Es poco preciso", "Se ve borroso"],
                "correcta": "Es lento",
                "retro_acierto": "¬°Correcto! Tiene que comparar contra TODO cada vez.",
                "retro_fallo": "Incorrecto. El problema es la velocidad, porque tiene que medir distancia con todos."
            },
            "versus": {
                "rival": "CNN (Redes Neuronales)",
                "comparacion": "CNN aprende formas (l√≠neas, curvas). KNN solo compara p√≠xeles brutos."
            }
        },
        {
            "titulo": "üç∑ 3. Calidad de Vinos",
            "contexto": "Clasificar vino compar√°ndolo con botellas similares qu√≠micas.",
            "pasos": ["1. Datos qu√≠micos.", "2. Buscar gemelos.", "3. Calidad."],
            "codigo": """from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# [Acidez, Az√∫car]
X = np.array([[7, 2], [6, 1.5], [8, 5], [9, 6]]) 
y = np.array(['Bueno', 'Bueno', 'Malo', 'Malo'])

modelo = KNeighborsClassifier(n_neighbors=3)
modelo.fit(X, y)

vino_nuevo = [[6.5, 2]]
print("Calidad:", modelo.predict(vino_nuevo)[0])""",
            
            "explicacion_codigo": "Se llama 'Lazy Learning' (Aprendizaje Vago) porque no genera una f√≥rmula, solo consulta la base de datos en el momento.",
            "quiz": {
                "pregunta": "¬øKNN crea una f√≥rmula matem√°tica?",
                "opciones": ["S√≠", "No (Lazy)", "A veces"],
                "correcta": "No (Lazy)",
                "retro_acierto": "¬°Muy bien! Solo guarda datos en memoria.",
                "retro_fallo": "Falso. KNN no genera una ecuaci√≥n matem√°tica, solo almacena los puntos."
            },
            "versus": {
                "rival": "SVM",
                "comparacion": "SVM busca una f√≥rmula general. KNN carga con todos los datos."
            }
        }
    ],

    "Random Forest": [
        {
            "titulo": "üè• 1. Diagn√≥stico M√©dico",
            "contexto": "Votaci√≥n de m√∫ltiples 'doctores' (√°rboles).",
            "pasos": ["1. Muchos √°rboles.", "2. Votaci√≥n.", "3. Mayor√≠a gana."],
            "codigo": """from sklearn.ensemble import RandomForestClassifier
import numpy as np

# [Edad, Colesterol, Presi√≥n]
X = np.array([
    [25, 180, 120], 
    [30, 240, 140], 
    [50, 200, 130], 
    [60, 260, 150]
])
y = np.array(['Sano', 'Enfermo', 'Sano', 'Enfermo'])

# 10 √°rboles votando
modelo = RandomForestClassifier(n_estimators=10, random_state=42)
modelo.fit(X, y)

paciente = [[40, 250, 135]]
print("Diagn√≥stico:", modelo.predict(paciente)[0])""",
            
            "explicacion_codigo": "n_estimators=10 crea 10 √°rboles distintos. Cada uno ve una parte diferente de los datos para evitar sesgos.",
            "quiz": {
                "pregunta": "¬øVentaja frente a un solo √°rbol?",
                "opciones": ["M√°s bonito", "Evita errores (Overfitting)", "M√°s r√°pido"],
                "correcta": "Evita errores (Overfitting)",
                "retro_acierto": "¬°Exacto! El bosque promedia errores individuales.",
                "retro_fallo": "No. Un solo √°rbol suele ser m√°s r√°pido, pero el bosque se equivoca menos."
            },
            "versus": {
                "rival": "Decision Tree",
                "comparacion": "Un √°rbol es inestable. El bosque es robusto y fiable."
            }
        },
        {
            "titulo": "ü¶Å 2. Clasificaci√≥n de Animales",
            "contexto": "Reglas complejas: Pelo, huevos, vuela.",
            "pasos": ["1. Caracter√≠sticas.", "2. Bosque decide.", "3. Especie."],
            "codigo": """from sklearn.ensemble import RandomForestClassifier
import numpy as np

# [Pelo?, Huevos?, Vuela?]
X = np.array([
    [1,0,0], # Perro
    [0,1,1], # P√°jaro
    [0,1,0]  # Serpiente
])
y = np.array(['Mam√≠fero', 'Ave', 'Reptil'])

modelo = RandomForestClassifier(n_estimators=10)
modelo.fit(X, y)

print("Animal:", modelo.predict([[1, 0, 0]])[0])""",
            
            "explicacion_codigo": "Cada √°rbol hace preguntas tipo '¬øTiene pelo?'. Al final, si 8 √°rboles dicen 'Mam√≠fero', esa es la respuesta.",
            "quiz": {
                "pregunta": "¬øC√≥mo se llama combinar modelos?",
                "opciones": ["Ensemble Learning", "Deep Learning", "Cluster"],
                "correcta": "Ensemble Learning",
                "retro_acierto": "¬°Bien! Ensemble significa conjunto. La uni√≥n hace la fuerza.",
                "retro_fallo": "Incorrecto. Se llama Aprendizaje en Conjunto (Ensemble)."
            },
            "versus": {
                "rival": "Redes Neuronales",
                "comparacion": "Las redes son cajas negras. Random Forest te dice qu√© variables importaron."
            }
        },
        {
            "titulo": "üí≥ 3. Fraude Bancario",
            "contexto": "Detectar transacciones raras con muchas variables.",
            "pasos": ["1. Historial.", "2. Votaci√≥n masiva.", "3. Alerta."],
            "codigo": """from sklearn.ensemble import RandomForestClassifier
import numpy as np

# [Monto, Hora, Distancia]
X = np.array([
    [10, 12, 1], 
    [10000, 3, 500], # Fraude obvio
    [20, 14, 2]
])
y = np.array(['Ok', 'Fraude', 'Ok'])

modelo = RandomForestClassifier(n_estimators=100)
modelo.fit(X, y)

transaccion = [[5000, 4, 200]]
print("Estado:", modelo.predict(transaccion)[0])""",
            
            "explicacion_codigo": "Usamos 100 √°rboles porque el fraude es sutil. Necesitamos muchas 'opiniones' para estar seguros.",
            "quiz": {
                "pregunta": "¬øSi un √°rbol se equivoca?",
                "opciones": ["Falla todo", "Los otros 99 corrigen", "Se borra"],
                "correcta": "Los otros 99 corrigen",
                "retro_acierto": "¬°Esa es la clave! La sabidur√≠a de la mayor√≠a gana.",
                "retro_fallo": "No. Si uno falla, los otros 99 ganan la votaci√≥n."
            },
            "versus": {
                "rival": "Gradient Boosting",
                "comparacion": "RF entrena √°rboles en paralelo. Boosting los entrena en serie corrigiendo errores previos."
            }
        }
    ],

    # =========================================================
    # CLUSTERING (Agrupar)
    # =========================================================

    "K-Means": [
        {
            "titulo": "üëï 1. Tallas de Camisetas",
            "contexto": "Definir tallas S, M, L en datos de medidas.",
            "pasos": ["1. Definir K=3.", "2. Mover centros.", "3. Asignar grupos."],
            "codigo": """from sklearn.cluster import KMeans
import numpy as np

# [Altura, Peso]
X = np.array([
    [160, 55], [165, 60], # Peque√±os
    [180, 80], [185, 85], # Grandes
    [175, 70]             # Medios
])

# K=3 (Queremos 3 tallas)
modelo = KMeans(n_clusters=3, n_init=10)
modelo.fit(X)

print("Centros (Tallas ideales):", modelo.cluster_centers_)
print("Grupo cliente nuevo:", modelo.predict([[170, 72]])[0])""",
            
            "explicacion_codigo": "n_clusters=3 es obligatorio. El algoritmo mueve 3 puntos centrales hasta que quedan en medio de los grupos de datos.",
            "quiz": {
                "pregunta": "¬øQu√© debes decirle a K-Means obligatoriamente?",
                "opciones": ["Nombres", "N√∫mero de grupos (K)", "Colores"],
                "correcta": "N√∫mero de grupos (K)",
                "retro_acierto": "¬°Correcto! K-Means no sabe adivinar cu√°ntos grupos hay.",
                "retro_fallo": "Falso. El algoritmo necesita saber cu√°ntos grupos (K) buscar antes de empezar."
            },
            "versus": {
                "rival": "DBSCAN",
                "comparacion": "K-Means te obliga a elegir K. DBSCAN encuentra el n√∫mero solo."
            }
        },
        {
            "titulo": "üé® 2. Compresi√≥n de Im√°genes",
            "contexto": "Reducir colores de una foto a solo 2.",
            "pasos": ["1. P√≠xeles RGB.", "2. K=2.", "3. Reducir."],
            "codigo": """from sklearn.cluster import KMeans
import numpy as np

# P√≠xeles [R, G, B]
X = np.array([
    [255, 0, 0], [250, 10, 10], # Rojos
    [0, 0, 255], [10, 10, 250]  # Azules
])

modelo = KMeans(n_clusters=2, n_init=10)
modelo.fit(X)

print("Etiquetas (0 o 1):", modelo.labels_)""",
            
            "explicacion_codigo": "Agrupa miles de colores en solo 2 promedios. As√≠ se comprimen las im√°genes (GIF, PNG).",
            "quiz": {
                "pregunta": "¬øC√≥mo se llama el centro del grupo?",
                "opciones": ["Centroide", "N√∫cleo", "L√≠der"],
                "correcta": "Centroide",
                "retro_acierto": "¬°Bien! Es el promedio matem√°tico.",
                "retro_fallo": "No. En K-Means se le llama Centroide."
            },
            "versus": {
                "rival": "Hierarchical Clustering",
                "comparacion": "K-Means es r√°pido. Hierarchical permite ver subgrupos dentro de grupos."
            }
        },
        {
            "titulo": "üë• 3. Segmentaci√≥n de Clientes",
            "contexto": "Agrupar por comportamiento de compra.",
            "pasos": ["1. Datos sin etiqueta.", "2. Agrupar.", "3. Analizar."],
            "codigo": """from sklearn.cluster import KMeans
import numpy as np

# [Gasto Anual, Frecuencia]
X = np.array([[1000, 50], [200, 2], [1200, 60], [150, 5]])

modelo = KMeans(n_clusters=2, n_init=10)
modelo.fit(X)

cliente = [[500, 20]]
print("Grupo asignado:", modelo.predict(cliente)[0])""",
            
            "explicacion_codigo": "El modelo detecta patrones: 'Gente que gasta mucho' vs 'Gente que gasta poco', sin que t√∫ se lo digas.",
            "quiz": {
                "pregunta": "¬øTipo de aprendizaje?",
                "opciones": ["Supervisado", "No Supervisado", "Reforzado"],
                "correcta": "No Supervisado",
                "retro_acierto": "¬°Exacto! No usamos etiquetas.",
                "retro_fallo": "Incorrecto. Como no le damos las respuestas correctas, es No Supervisado."
            },
            "versus": {
                "rival": "Clasificaci√≥n",
                "comparacion": "En Clasificaci√≥n t√∫ ense√±as las clases. En Clustering el modelo las inventa."
            }
        }
    ],

    "DBSCAN": [
        {
            "titulo": "üó∫Ô∏è 1. Zonas de Calor (GPS)",
            "contexto": "Encontrar grupos densos e ignorar ruido.",
            "pasos": ["1. GPS.", "2. Densidad.", "3. Ruido."],
            "codigo": """from sklearn.cluster import DBSCAN
import numpy as np

# Coordenadas
X = np.array([
    [1, 1], [1, 2], [2, 1], # Grupo denso
    [100, 100]              # Ruido (Lejos)
])

# eps=3 (distancia m√°xima), min_samples=2 (vecinos m√≠nimos)
modelo = DBSCAN(eps=3, min_samples=2)
labels = modelo.fit_predict(X)

print("Etiquetas:", labels) 
# 0 = Grupo, -1 = Ruido""",
            
            "explicacion_codigo": "DBSCAN es √∫nico porque etiqueta como '-1' los datos que est√°n solos y lejos. K-Means los forzar√≠a a entrar en un grupo.",
            "quiz": {
                "pregunta": "¬øQu√© hace con los puntos aislados?",
                "opciones": ["Los fuerza", "Marca Ruido (-1)", "Borra"],
                "correcta": "Marca Ruido (-1)",
                "retro_acierto": "¬°Correcto! Limpia datos sucios.",
                "retro_fallo": "No. No los borra ni los fuerza, los etiqueta diferente (-1)."
            },
            "versus": {
                "rival": "K-Means",
                "comparacion": "K-Means asume esferas. DBSCAN encuentra formas raras (serpientes, lunas)."
            }
        },
        {
            "titulo": "üí≥ 2. Detecci√≥n de Anomal√≠as",
            "contexto": "Transacci√≥n solitaria = Fraude.",
            "pasos": ["1. Transacciones.", "2. DBSCAN.", "3. Alerta."],
            "codigo": """from sklearn.cluster import DBSCAN
import numpy as np

X = np.array([[10, 10], [11, 11], [10, 12], [500, 500]])

modelo = DBSCAN(eps=5, min_samples=2)
labels = modelo.fit_predict(X)

es_anomalia = labels[-1] == -1
print("¬øEl √∫ltimo es anomal√≠a?:", es_anomalia)""",
            
            "explicacion_codigo": "Si no tiene suficientes vecinos cerca (definido por eps y min_samples), se considera una anomal√≠a.",
            "quiz": {
                "pregunta": "¬øNecesitas decirle cu√°ntos grupos (K) buscar?",
                "opciones": ["S√≠", "No", "A veces"],
                "correcta": "No",
                "retro_acierto": "¬°Exacto! √âl te dir√° cu√°ntos encontr√≥.",
                "retro_fallo": "Falso. DBSCAN descubre el n√∫mero de grupos autom√°ticamente."
            },
            "versus": {
                "rival": "Isolation Forest",
                "comparacion": "Isolation Forest es solo para anomal√≠as. DBSCAN agrupa Y detecta anomal√≠as."
            }
        },
        {
            "titulo": "‚ú® 3. Astronom√≠a (Galaxias)",
            "contexto": "Agrupar estrellas.",
            "pasos": ["1. Estrellas.", "2. Densidad.", "3. Grupos."],
            "codigo": """from sklearn.cluster import DBSCAN
import numpy as np

X = np.array([[10,10], [12,12], [80,80], [82,82], [50,50]])

modelo = DBSCAN(eps=10, min_samples=2)
labels = modelo.fit_predict(X)

print("Grupos encontrados:", set(labels))""",
            
            "explicacion_codigo": "Conecta estrellas cercanas 'saltando' de una a otra. As√≠ puede dibujar la forma de una galaxia irregular.",
            "quiz": {
                "pregunta": "¬øQu√© es Epsilon (eps)?",
                "opciones": ["Radio b√∫squeda", "Estrellas", "Velocidad"],
                "correcta": "Radio b√∫squeda",
                "retro_acierto": "¬°Bien! Define la vecindad.",
                "retro_fallo": "Incorrecto. Epsilon es la distancia m√°xima para considerar dos puntos como vecinos."
            },
            "versus": {
                "rival": "Gaussian Mixture",
                "comparacion": "GMM usa probabilidad. DBSCAN usa densidad pura."
            }
        }
    ]
}